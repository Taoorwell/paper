{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from osgeo import gdal, gdalconst\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = gdal.GetDriverByName(\"HFA\")\n",
    "driver.Register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 301 4\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "needclass_file = r\"G:\\Paper\\JL\\GF-2_3000_GS_302.dat\"\n",
    "ds = gdal.Open(needclass_file)\n",
    "# getting dimensions\n",
    "cols = ds.RasterXSize\n",
    "rows = ds.RasterYSize\n",
    "bands = ds.RasterCount\n",
    "print(cols, rows, bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature files pixel by pixel\n",
    "needclassdsbypixel = []\n",
    "for i in range(0, rows, 1):\n",
    "    for j in range(0, cols, 1):\n",
    "        data = ds.ReadAsArray(j, i, 1, 1).reshape(4,)/10000\n",
    "        needclassdsbypixel.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0465  0.0637  0.0484  0.3027] (4,)\n"
     ]
    }
   ],
   "source": [
    "print( needclassdsbypixel[0],  needclassdsbypixel[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from G:\\Paper\\python\\TensorFlow\\test06\\net_mlp_lr==le-3_100_kp==0.5--/my_net.ckpt\n",
      "[[  3.47179685e-34   3.84495761e-05   7.21722758e-22   4.25678728e-11\n",
      "    9.99961495e-01]\n",
      " [  4.12200496e-35   5.26833765e-06   1.77100601e-21   1.82080739e-11\n",
      "    9.99994755e-01]\n",
      " [  8.22057839e-36   3.56233407e-08   2.11635573e-21   8.07170411e-13\n",
      "    1.00000000e+00]\n",
      " ..., \n",
      " [  4.17747223e-08   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    9.33258784e-16]\n",
      " [  1.90497867e-10   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.96894655e-17]\n",
      " [  3.39131015e-11   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    3.87664786e-19]]\n"
     ]
    }
   ],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        tf.summary.histogram(\"histogram\", var)        \n",
    "# make placeholder\n",
    "with tf.name_scope(\"input\"):\n",
    "    x = tf.placeholder(tf.float32, [None, 4],name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None, 5],name=\"y\")\n",
    "    # keep_prob define dropout parameter\n",
    "    keep_prob = tf.placeholder(tf.float32) \n",
    "# neruon network framework\n",
    "with tf.name_scope(\"hlayer1\"):\n",
    "    with tf.name_scope(\"w1\"):\n",
    "        w1 = tf.Variable(tf.truncated_normal([4,100], stddev=0.1))\n",
    "        variable_summaries(w1)\n",
    "    with tf.name_scope(\"b1\"):\n",
    "        b1 = tf.Variable(tf.zeros([100])+0.1)\n",
    "        variable_summaries(b1)\n",
    "    L1 = tf.nn.tanh(tf.matmul(x, w1)+b1)\n",
    "    L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "with tf.name_scope(\"hlayer2\"):\n",
    "    with tf.name_scope(\"w2\"):\n",
    "        w2 = tf.Variable(tf.truncated_normal([100,100], stddev=0.1))\n",
    "        variable_summaries(w2)\n",
    "    with tf.name_scope(\"b2\"):\n",
    "        b2 = tf.Variable(tf.zeros([100])+0.1)\n",
    "        variable_summaries(b2)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1_dropout, w2)+b2)\n",
    "    L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "with tf.name_scope(\"sotfmax\"):\n",
    "    with tf.name_scope(\"w3\"):\n",
    "        w3 = tf.Variable(tf.truncated_normal([100,5], stddev=0.1))\n",
    "        variable_summaries(w3)\n",
    "    with tf.name_scope(\"b3\"):\n",
    "        b3 = tf.Variable(tf.zeros([5])+0.1)\n",
    "        variable_summaries(b3)\n",
    "    prediction = tf.nn.softmax(tf.matmul(L2_dropout, w3)+b3)\n",
    "# initial variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,r\"G:\\Paper\\python\\TensorFlow\\test06\\net_mlp_lr==le-3_100_kp==0.5--/my_net.ckpt\")\n",
    "    da_pre = sess.run(prediction, feed_dict= {x:needclassdsbypixel,keep_prob:1.0})\n",
    "    print(da_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4, ..., 4, 4, 4],\n",
       "       [4, 4, 4, ..., 4, 4, 4],\n",
       "       [4, 4, 4, ..., 4, 4, 4],\n",
       "       ..., \n",
       "       [4, 4, 4, ..., 2, 2, 2],\n",
       "       [4, 4, 4, ..., 2, 2, 2],\n",
       "       [4, 4, 4, ..., 2, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result1 = np.argmax(da_pre, axis=-1)\n",
    "class_result2 = class_result1.reshape(301, 301) \n",
    "class_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDataset = driver.Create(r\"G:\\Paper\\JL\\test_res\\302_mlp_5_cl.img\",\n",
    "                          301, 301, 1, gdal.GDT_Int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1],\n",
       "       [4, 4, 4, 4, 4, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1],\n",
       "       [4, 4, 4, 4, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1],\n",
       "       [4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 1, 1, 1, 4, 1, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outBand = outDataset.GetRasterBand(1)\n",
    "outBand.WriteArray(class_result2,0, 0)\n",
    "A = outBand.ReadAsArray(0, 0, 20, 20)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 4.0, 2.4770035650820685, 1.4224792331211429]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outBand.FlushCache()\n",
    "outBand.GetStatistics(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoTransform = ds.GetGeoTransform()\n",
    "outDataset.SetGeoTransform(geoTransform)\n",
    "proj = ds.GetProjection()\n",
    "outDataset.SetProjection(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building pyramids\n",
    "gdal.SetConfigOption(\"HFA_USE_RRD\", \"YES\")\n",
    "outDataset.BuildOverviews(overviewlist=[2, 4, 8, 16, 32, 64, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543616.671\n",
      "PROJCS[\"WGS_1984_UTM_Zone_50N\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"WGS_1984\",SPHEROID[\"WGS_84\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",117.0],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1]]\n"
     ]
    }
   ],
   "source": [
    "geotransform = outDataset.GetGeoTransform()\n",
    "print(geotransform[0])\n",
    "proj = outDataset.GetProjection()\n",
    "print(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
