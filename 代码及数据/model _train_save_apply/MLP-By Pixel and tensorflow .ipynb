{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, gdalconst\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = gdal.GetDriverByName(\"HFA\")\n",
    "driver.Register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load feature files from folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load data by pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_pixel(filepath):\n",
    "    filename_list = listdir(filepath)\n",
    "    f = []\n",
    "    for filename in filename_list:\n",
    "        if filename.endswith(\".dat\"):\n",
    "            f.append(filepath+\"\\\\\"+filename)\n",
    "    dsbypixel = []\n",
    "    for feature_file in f:\n",
    "        ds = gdal.Open(feature_file)\n",
    "        cols = ds.RasterXSize\n",
    "        rows = ds.RasterYSize\n",
    "        bands = ds.RasterCount\n",
    "        for i in range(0, rows, 1):\n",
    "            for j in range(0, cols, 1):\n",
    "                data = ds.ReadAsArray(j, i,1,1).reshape(4,)/10000\n",
    "                dsbypixel.append(data)\n",
    "    return dsbypixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shrub nums: 2178\n",
      "Masson's pine nums: 2669\n",
      "Other broadleaf nums: 2413\n",
      "Bare soil nums: 2228\n",
      "Chinese fir nums: 3686\n",
      "13174\n",
      "[ 1.  1.  1. ...,  5.  5.  5.] (13174,)\n"
     ]
    }
   ],
   "source": [
    "h = [\"Shrub\", \"Masson's pine\",\"Other broadleaf\",\"Bare soil\",\"Chinese fir\"]\n",
    "dsbypixel_s = []\n",
    "pixel_labels = []\n",
    "j = 1\n",
    "for i in h:\n",
    "    a = load_data_pixel(r\"G:\\Paper\\python\\TensorFlow\\test04\\dataset\"+\"\\\\\"+i)\n",
    "    dsbypixel_s.extend(a)\n",
    "    b = np.ones([len(a),])*j\n",
    "    pixel_labels = np.hstack((pixel_labels,b))\n",
    "    j = j+1\n",
    "    print(i+\" nums: \"+ str(len(a)))\n",
    "print(len(dsbypixel_s))\n",
    "print(pixel_labels, pixel_labels.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot for labels dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels for pixel datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "(13174, 5)\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "pixel_labels_onehot = onehot_encoder.fit_transform(pixel_labels.reshape(-1, 1))\n",
    "print(pixel_labels_onehot)\n",
    "print(pixel_labels_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_dataset = {}\n",
    "pixel_dataset['feature'] = np.array(dsbypixel_s)\n",
    "pixel_dataset['target'] = pixel_labels_onehot\n",
    "pixel_dataset['target name'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': array([[ 0.059 ,  0.1003,  0.0693,  0.4776],\n",
      "       [ 0.058 ,  0.0988,  0.067 ,  0.4698],\n",
      "       [ 0.0573,  0.0982,  0.0655,  0.4635],\n",
      "       ..., \n",
      "       [ 0.0484,  0.0657,  0.056 ,  0.3388],\n",
      "       [ 0.0495,  0.067 ,  0.0581,  0.3448],\n",
      "       [ 0.0514,  0.0697,  0.0617,  0.3577]]), 'target': array([[ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]]), 'target name': ['brush', 'maweisong', 'qitakuoye', 'road', 'shamu']}\n"
     ]
    }
   ],
   "source": [
    "print(pixel_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation =train_test_split(pixel_dataset[\"feature\"], pixel_dataset[\"target\"],\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0508  0.0847  0.0589  0.4909] (9880, 4) [ 1.  0.  0.  0.  0.] (9880, 5)\n",
      "[ 0.0454  0.0707  0.0546  0.4112] (3294, 4) [ 0.  0.  1.  0.  0.] (3294, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1],x_train.shape, y_train[1],y_train.shape)\n",
    "print(x_validation[1],x_validation.shape, y_validation[1],y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     10, Train Accuracy:  91.0%, Validation Acc:  91.9% *\n",
      "Iter:     20, Train Accuracy:  93.0%, Validation Acc:  93.4% *\n",
      "Iter:     30, Train Accuracy:  93.4%, Validation Acc:  93.8% *\n",
      "Iter:     40, Train Accuracy:  93.8%, Validation Acc:  94.2% *\n",
      "Iter:     50, Train Accuracy:  93.9%, Validation Acc:  94.4% *\n",
      "Iter:     60, Train Accuracy:  94.3%, Validation Acc:  94.7% *\n",
      "Iter:     70, Train Accuracy:  94.5%, Validation Acc:  95.0% *\n",
      "Iter:     80, Train Accuracy:  94.7%, Validation Acc:  94.9%  \n",
      "Iter:     90, Train Accuracy:  95.0%, Validation Acc:  95.2% *\n",
      "Iter:    100, Train Accuracy:  94.8%, Validation Acc:  95.1%  \n",
      "Iter:    110, Train Accuracy:  95.1%, Validation Acc:  95.2%  \n",
      "Iter:    120, Train Accuracy:  95.1%, Validation Acc:  95.3% *\n",
      "Iter:    130, Train Accuracy:  95.4%, Validation Acc:  95.6% *\n",
      "Iter:    140, Train Accuracy:  95.5%, Validation Acc:  96.1% *\n",
      "Iter:    150, Train Accuracy:  94.9%, Validation Acc:  95.4%  \n",
      "Iter:    160, Train Accuracy:  95.9%, Validation Acc:  96.3% *\n",
      "Iter:    170, Train Accuracy:  95.8%, Validation Acc:  96.1%  \n",
      "Iter:    180, Train Accuracy:  96.0%, Validation Acc:  96.6% *\n",
      "Iter:    190, Train Accuracy:  95.9%, Validation Acc:  96.5%  \n",
      "Iter:    200, Train Accuracy:  96.2%, Validation Acc:  96.7% *\n",
      "Iter:    210, Train Accuracy:  96.1%, Validation Acc:  96.3%  \n",
      "Iter:    220, Train Accuracy:  96.4%, Validation Acc:  96.8% *\n",
      "Iter:    230, Train Accuracy:  96.2%, Validation Acc:  96.5%  \n",
      "Iter:    240, Train Accuracy:  95.7%, Validation Acc:  96.2%  \n",
      "Iter:    250, Train Accuracy:  96.7%, Validation Acc:  97.1% *\n",
      "Iter:    260, Train Accuracy:  96.4%, Validation Acc:  96.9%  \n",
      "Iter:    270, Train Accuracy:  96.8%, Validation Acc:  97.4% *\n",
      "Iter:    280, Train Accuracy:  96.8%, Validation Acc:  97.3%  \n",
      "Iter:    290, Train Accuracy:  96.9%, Validation Acc:  97.3%  \n",
      "Iter:    300, Train Accuracy:  96.0%, Validation Acc:  96.4%  \n",
      "Iter:    310, Train Accuracy:  96.4%, Validation Acc:  96.8%  \n",
      "Iter:    320, Train Accuracy:  97.0%, Validation Acc:  97.4%  \n",
      "Iter:    330, Train Accuracy:  97.1%, Validation Acc:  97.4% *\n",
      "Iter:    340, Train Accuracy:  96.9%, Validation Acc:  97.3%  \n",
      "Iter:    350, Train Accuracy:  97.2%, Validation Acc:  97.7% *\n",
      "Iter:    360, Train Accuracy:  96.8%, Validation Acc:  97.3%  \n",
      "Iter:    370, Train Accuracy:  96.6%, Validation Acc:  96.9%  \n",
      "Iter:    380, Train Accuracy:  97.4%, Validation Acc:  97.8% *\n",
      "Iter:    390, Train Accuracy:  97.0%, Validation Acc:  97.5%  \n",
      "Iter:    400, Train Accuracy:  97.4%, Validation Acc:  97.6%  \n",
      "Iter:    410, Train Accuracy:  97.7%, Validation Acc:  98.1% *\n",
      "Iter:    420, Train Accuracy:  97.8%, Validation Acc:  98.1%  \n",
      "Iter:    430, Train Accuracy:  97.9%, Validation Acc:  98.3% *\n",
      "Iter:    440, Train Accuracy:  98.2%, Validation Acc:  98.6% *\n",
      "Iter:    450, Train Accuracy:  97.7%, Validation Acc:  97.9%  \n",
      "Iter:    460, Train Accuracy:  98.2%, Validation Acc:  98.5%  \n",
      "Iter:    470, Train Accuracy:  97.9%, Validation Acc:  98.4%  \n",
      "Iter:    480, Train Accuracy:  97.5%, Validation Acc:  97.8%  \n",
      "Iter:    490, Train Accuracy:  98.4%, Validation Acc:  98.7% *\n",
      "Iter:    500, Train Accuracy:  98.3%, Validation Acc:  98.5%  \n",
      "Iter:    510, Train Accuracy:  97.5%, Validation Acc:  97.7%  \n",
      "Iter:    520, Train Accuracy:  98.1%, Validation Acc:  98.5%  \n",
      "Iter:    530, Train Accuracy:  98.5%, Validation Acc:  98.7%  \n",
      "Iter:    540, Train Accuracy:  98.2%, Validation Acc:  98.4%  \n",
      "Iter:    550, Train Accuracy:  98.5%, Validation Acc:  98.8% *\n",
      "Iter:    560, Train Accuracy:  98.7%, Validation Acc:  98.7%  \n",
      "Iter:    570, Train Accuracy:  98.5%, Validation Acc:  98.7%  \n",
      "Iter:    580, Train Accuracy:  98.3%, Validation Acc:  98.5%  \n",
      "Iter:    590, Train Accuracy:  97.5%, Validation Acc:  97.4%  \n",
      "Iter:    600, Train Accuracy:  98.7%, Validation Acc:  98.8% *\n",
      "Iter:    610, Train Accuracy:  98.9%, Validation Acc:  98.9% *\n",
      "Iter:    620, Train Accuracy:  98.8%, Validation Acc:  99.0% *\n",
      "Iter:    630, Train Accuracy:  98.8%, Validation Acc:  98.9%  \n",
      "Iter:    640, Train Accuracy:  98.6%, Validation Acc:  98.7%  \n",
      "Iter:    650, Train Accuracy:  98.0%, Validation Acc:  98.0%  \n",
      "Iter:    660, Train Accuracy:  97.6%, Validation Acc:  97.7%  \n",
      "Iter:    670, Train Accuracy:  98.8%, Validation Acc:  99.1% *\n",
      "Iter:    680, Train Accuracy:  98.8%, Validation Acc:  98.8%  \n",
      "Iter:    690, Train Accuracy:  98.7%, Validation Acc:  98.6%  \n",
      "Iter:    700, Train Accuracy:  98.5%, Validation Acc:  98.3%  \n",
      "Iter:    710, Train Accuracy:  98.4%, Validation Acc:  98.5%  \n",
      "Iter:    720, Train Accuracy:  98.6%, Validation Acc:  98.8%  \n",
      "Iter:    730, Train Accuracy:  99.0%, Validation Acc:  99.3% *\n",
      "Iter:    740, Train Accuracy:  98.7%, Validation Acc:  98.8%  \n",
      "Iter:    750, Train Accuracy:  98.9%, Validation Acc:  98.8%  \n",
      "Iter:    760, Train Accuracy:  99.1%, Validation Acc:  99.2%  \n",
      "Iter:    770, Train Accuracy:  98.7%, Validation Acc:  98.8%  \n",
      "Iter:    780, Train Accuracy:  99.1%, Validation Acc:  99.2%  \n",
      "Iter:    790, Train Accuracy:  98.5%, Validation Acc:  98.2%  \n",
      "Iter:    800, Train Accuracy:  98.9%, Validation Acc:  99.0%  \n",
      "Iter:    810, Train Accuracy:  98.3%, Validation Acc:  98.2%  \n",
      "Iter:    820, Train Accuracy:  99.0%, Validation Acc:  99.0%  \n",
      "Iter:    830, Train Accuracy:  97.9%, Validation Acc:  98.1%  \n",
      "No improvement found in a while, stopping optimization\n"
     ]
    }
   ],
   "source": [
    "#load mnist dataset\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "# make batches\n",
    "batch_size = 50\n",
    "n_batch = x_train.shape[0] // batch_size\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        tf.summary.histogram(\"histogram\", var)\n",
    "        \n",
    "# make placeholder\n",
    "with tf.name_scope(\"input\"):\n",
    "    x = tf.placeholder(tf.float32, [None, 4],name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None, 5],name=\"y\")\n",
    "# keep_prob define dropout parameter\n",
    "    keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "# neruon network framework\n",
    "with tf.name_scope(\"hlayer1\"):\n",
    "    with tf.name_scope(\"w1\"):\n",
    "        w1 = tf.Variable(tf.truncated_normal([4,100], stddev=0.1))\n",
    "        variable_summaries(w1)\n",
    "    with tf.name_scope(\"b1\"):\n",
    "        b1 = tf.Variable(tf.zeros([100])+0.1)\n",
    "        variable_summaries(b1)\n",
    "    L1 = tf.nn.tanh(tf.matmul(x, w1)+b1)\n",
    "    L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "with tf.name_scope(\"hlayer2\"):\n",
    "    with tf.name_scope(\"w2\"):\n",
    "        w2 = tf.Variable(tf.truncated_normal([100,100], stddev=0.1))\n",
    "        variable_summaries(w2)\n",
    "    with tf.name_scope(\"b2\"):\n",
    "        b2 = tf.Variable(tf.zeros([100])+0.1)\n",
    "        variable_summaries(b2)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1_dropout, w2)+b2)\n",
    "    L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "with tf.name_scope(\"sotfmax\"):\n",
    "    with tf.name_scope(\"w3\"):\n",
    "        w3 = tf.Variable(tf.truncated_normal([100,5], stddev=0.1))\n",
    "        variable_summaries(w3)\n",
    "    with tf.name_scope(\"b3\"):\n",
    "        b3 = tf.Variable(tf.zeros([5])+0.1)\n",
    "        variable_summaries(b3)\n",
    "    prediction = tf.nn.softmax(tf.matmul(L2_dropout, w3)+b3)\n",
    "\n",
    "# cross-entropy\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "# Change the optimizer\n",
    "with tf.name_scope(\"train\"):  \n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "# initial variables\n",
    "init = tf.global_variables_initializer()\n",
    "# accuracy\n",
    "with tf.name_scope(\"accuracy\"): \n",
    "    with tf.name_scope(\"correct_prediction\"):  \n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "    with tf.name_scope(\"accuracy\"): \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "merge = tf.summary.merge_all()\n",
    "train_acc_summary = tf.summary.scalar(\"train_accuracy\",accuracy)\n",
    "validation_acc_summary = tf.summary.scalar(\"validation\", accuracy)\n",
    "best_validation_accuracy = 0.0\n",
    "last_improvement = 0\n",
    "require_improvement = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(r\"G:\\Paper\\python\\TensorFlow\\test06\\logfile/mlp_lr==1e-3_100_kp==0.5\", sess.graph)\n",
    "    best_validation_accuracy = 0.0\n",
    "    last_improvement = 0\n",
    "    require_improvement = 100\n",
    "    total_iterations = 0\n",
    "    for epoch in range(1000):\n",
    "        idxs = np.arange(x_train.shape[0])\n",
    "        x_random = x_train[idxs]\n",
    "        y_random = y_train[idxs]\n",
    "        total_iterations += 1\n",
    "        for i in range(n_batch):\n",
    "            batch_x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            summary,_ = sess.run([merge,train_step], feed_dict={x:batch_x, y:batch_y, keep_prob:0.5})\n",
    "        train_acc_str = sess.run(train_acc_summary, feed_dict={x:x_train, y:y_train, keep_prob:1.0})\n",
    "        validation_acc_str = sess.run(validation_acc_summary,feed_dict={x:x_validation, y:y_validation, keep_prob:1.0})\n",
    "        writer.add_summary(train_acc_str,epoch)\n",
    "        writer.add_summary(validation_acc_str, epoch)\n",
    "        writer.add_summary(summary,epoch)\n",
    "        if (total_iterations % 10 == 0) or (epoch == 999):\n",
    "            train_acc = sess.run(accuracy, feed_dict={x:x_train, y:y_train, keep_prob:1.0})\n",
    "            validation_acc = sess.run(accuracy, feed_dict={x:x_validation, y:y_validation, keep_prob:1.0})\n",
    "            if validation_acc > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_acc\n",
    "                last_improvement = total_iterations\n",
    "                saver.save(sess,r\"G:\\Paper\\python\\TensorFlow\\test06\\net_mlp_lr==le-3_100_kp==0.5--/my_net.ckpt\")\n",
    "                improved_str = \"*\"\n",
    "            else:\n",
    "                improved_str = \" \"\n",
    "            msg = \"Iter: {0:>6}, Train Accuracy: {1:>6.1%}, Validation Acc: {2:>6.1%} {3}\"\n",
    "            print(msg.format(epoch + 1, train_acc, validation_acc, improved_str))\n",
    "        if total_iterations - last_improvement > require_improvement:\n",
    "            print(\"No improvement found in a while, stopping optimization\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
