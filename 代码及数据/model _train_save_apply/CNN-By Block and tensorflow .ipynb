{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, gdalconst\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = gdal.GetDriverByName(\"HFA\")\n",
    "driver.Register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load feature files from folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load data by block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_block(filepath,m):\n",
    "    filename_list = listdir(filepath)\n",
    "    f = []\n",
    "    for filename in filename_list:\n",
    "        if filename.endswith(\".dat\"):\n",
    "            f.append(filepath+\"\\\\\"+filename)\n",
    "    dsbyblock = []\n",
    "    for feature_file in f:\n",
    "        ds = gdal.Open(feature_file)\n",
    "        cols = ds.RasterXSize\n",
    "        rows = ds.RasterYSize\n",
    "        bands = ds.RasterCount\n",
    "        for i in range(0, rows, 1):\n",
    "            if i + m <= rows:\n",
    "                for j in range(0, cols, 1):\n",
    "                    if j + m <= cols:\n",
    "                        data = ds.ReadAsArray(j, i, m, m).reshape(m*m*4,)/10000\n",
    "                        dsbyblock.append(data)\n",
    "    return dsbyblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brush nums: 918\n",
      "maweisong nums: 1089\n",
      "qitakuoye nums: 1053\n",
      "road nums: 968\n",
      "shamu nums: 1366\n",
      "5394\n",
      "[ 1.  1.  1. ...,  5.  5.  5.] (5394,)\n"
     ]
    }
   ],
   "source": [
    "h = [\"Shrub\", \"Masson's pine\",\"Other broadleaf\",\"Bare soil\",\"Chinese fir\"]\n",
    "dsbyblock_ss = []\n",
    "blocks_labels = []\n",
    "m = 11\n",
    "j = 1\n",
    "for i in h:\n",
    "    a = load_data_block(r\"G:\\Paper\\python\\TensorFlow\\test04\\dataset\"+\"\\\\\"+i,m)\n",
    "    dsbyblock_ss.extend(a)\n",
    "    b = np.ones([len(a),])*j\n",
    "    blocks_labels = np.hstack((blocks_labels,b))\n",
    "    j = j + 1\n",
    "    print(i+\" nums: \"+ str(len(a)))\n",
    "print(len(dsbyblock_ss))\n",
    "print(blocks_labels, blocks_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #m = 7 \n",
    "# brush nums: 1350\n",
    "# maweisong nums: 1625\n",
    "# qitakuoye nums: 1525\n",
    "# road nums: 1400\n",
    "# shamu nums: 2150\n",
    "# 8050\n",
    "##m = 5\n",
    "# brush nums: 1602\n",
    "# maweisong nums: 1941\n",
    "# qitakuoye nums: 1797\n",
    "# road nums: 1652\n",
    "# shamu nums: 2614\n",
    "# 9606\n",
    "##m = 9\n",
    "# brush nums: 1122\n",
    "# maweisong nums: 1341\n",
    "# qitakuoye nums: 1277\n",
    "# road nums: 1172\n",
    "# shamu nums: 1734\n",
    "# 6646\n",
    "# m =11\n",
    "# brush nums: 918\n",
    "# maweisong nums: 1089\n",
    "# qitakuoye nums: 1053\n",
    "# road nums: 968\n",
    "# shamu nums: 1366\n",
    "# 5394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot for labels dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels for block datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "(5394, 5)\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "blocks_labels_onehot = onehot_encoder.fit_transform(blocks_labels.reshape(-1, 1))\n",
    "print(blocks_labels_onehot)\n",
    "print(blocks_labels_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_dataset = {}\n",
    "blocks_dataset['feature'] = np.array(dsbyblock_ss)\n",
    "blocks_dataset['target'] = blocks_labels_onehot\n",
    "blocks_dataset['target name'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': array([[ 0.059 ,  0.058 ,  0.0573, ...,  0.4484,  0.4367,  0.4294],\n",
      "       [ 0.058 ,  0.0573,  0.0579, ...,  0.4367,  0.4294,  0.4434],\n",
      "       [ 0.0573,  0.0579,  0.0584, ...,  0.4294,  0.4434,  0.4567],\n",
      "       ..., \n",
      "       [ 0.0494,  0.0476,  0.0479, ...,  0.3437,  0.3407,  0.3388],\n",
      "       [ 0.0476,  0.0479,  0.0477, ...,  0.3407,  0.3388,  0.3448],\n",
      "       [ 0.0479,  0.0477,  0.0465, ...,  0.3388,  0.3448,  0.3577]]), 'target': array([[ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]]), 'target name': ['brush', 'maweisong', 'qitakuoye', 'road', 'shamu']}\n"
     ]
    }
   ],
   "source": [
    "print(blocks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation =train_test_split(blocks_dataset[\"feature\"], blocks_dataset[\"target\"],\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4045, 484) [ 0.  0.  0.  0.  1.] (4045, 5)\n",
      "(1349, 484) [ 0.  0.  0.  1.  0.] (1349, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train[1],y_train.shape)\n",
    "print(x_validation.shape, y_validation[1],y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     10, Train Accuracy:  97.7%, Validation Acc:  96.7% *\n",
      "Iter:     20, Train Accuracy:  98.1%, Validation Acc:  97.7% *\n",
      "Iter:     30, Train Accuracy:  98.3%, Validation Acc:  98.1% *\n",
      "Iter:     40, Train Accuracy:  97.9%, Validation Acc:  97.7%  \n",
      "Iter:     50, Train Accuracy:  98.9%, Validation Acc:  99.0% *\n",
      "Iter:     60, Train Accuracy:  99.0%, Validation Acc:  99.1% *\n",
      "Iter:     70, Train Accuracy:  99.0%, Validation Acc:  99.1%  \n",
      "Iter:     80, Train Accuracy:  99.1%, Validation Acc:  99.2% *\n",
      "Iter:     90, Train Accuracy:  99.2%, Validation Acc:  99.3% *\n",
      "Iter:    100, Train Accuracy:  99.7%, Validation Acc:  99.9% *\n",
      "Iter:    110, Train Accuracy:  99.6%, Validation Acc:  99.5%  \n",
      "Iter:    120, Train Accuracy:  99.6%, Validation Acc:  99.3%  \n",
      "Iter:    130, Train Accuracy:  99.8%, Validation Acc:  99.5%  \n",
      "Iter:    140, Train Accuracy: 100.0%, Validation Acc:  99.9% *\n",
      "Iter:    150, Train Accuracy: 100.0%, Validation Acc: 100.0% *\n",
      "Iter:    160, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    170, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    180, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    190, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    200, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    210, Train Accuracy:  99.9%, Validation Acc:  99.5%  \n",
      "Iter:    220, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    230, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    240, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "Iter:    250, Train Accuracy: 100.0%, Validation Acc: 100.0%  \n",
      "No improvement found in a while, stopping optimization\n"
     ]
    }
   ],
   "source": [
    "# make batches\n",
    "batch_size = 10\n",
    "n_batch = x_train.shape[0] // batch_size\n",
    "# define function\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        tf.summary.histogram(\"histogram\", var)\n",
    "# init weights function\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# init biases function\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# define convo layer function\n",
    "def conv2d(x,W):\n",
    "    # x input tensor of shape [batch, in_height, in_width, in_channels]\n",
    "    # W filter/kernel tensor of shape [filter_height, filter_width, in_channels,out_channels]\n",
    "    #strides[0]=strides[3]=1,strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A \"string\" from:\"SAME\", \"VALID\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "def conv3d(x,W):\n",
    "    # x input tensor of shape [batch, in_depth,in_height, in_width, in_channels]\n",
    "    # W filter/kernel tensor of shape [filter_depth,filter_height, filter_width, in_channels,out_channels]\n",
    "    #strides[0]=strides[4]=1,strides[1]代表x方向的步长，strides[2]代表y方向的步长,strides[3]代表z方向的步长\n",
    "    #padding: A \"string\" from:\"SAME\", \"VALID\"\n",
    "    return tf.nn.conv3d(x, W, strides=[1, 1, 1, 1,1], padding=\"SAME\")\n",
    "# define pooling layer\n",
    "def max_pool_2x2(x):\n",
    "    #ksize(1, x, y, 1)\n",
    "    return tf.nn.max_pool(x,ksize=[1, 2, 2, 1], strides= [1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "# make placeholder\n",
    "    x = tf.placeholder(tf.float32, [None,m*m*4], name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None,5], name=\"y\")\n",
    "    x_image = tf.reshape(x, [-1,m,m, 4])\n",
    "with tf.name_scope(\"conv1\"):\n",
    "    # initial first conv layer weights and biases\n",
    "    with tf.name_scope(\"W_conv1\"):\n",
    "        W_conv1 = weight_variable([3, 3, 4, 16])\n",
    "        variable_summaries(W_conv1)\n",
    "    with tf.name_scope(\"b_conv1\"):\n",
    "        b_conv1 = bias_variable([16])\n",
    "        variable_summaries(b_conv1)\n",
    "    # x_image and weights biases  conv1, relu activation function\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    # initial second conv layer weights and biases\n",
    "    with tf.name_scope(\"W_conv2\"):\n",
    "        W_conv2 = weight_variable([3, 3, 16, 32])\n",
    "        variable_summaries(W_conv2)\n",
    "    with tf.name_scope(\"b_conv2\"):\n",
    "        b_conv2 = bias_variable([32])\n",
    "        variable_summaries(b_conv2)\n",
    "# h_pool1 and weights biases  conv2, relu activation function\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2) \n",
    "with tf.name_scope(\"fc1\"):\n",
    "    # initial first full-connectelly layer weights and biases\n",
    "    with tf.name_scope(\"W_fc1\"):\n",
    "        W_fc1 = weight_variable([3*3*32, 1000])\n",
    "        variable_summaries(W_fc1)\n",
    "    with tf.name_scope(\"b_fc1\"):\n",
    "        b_fc1 = bias_variable([1000])\n",
    "        variable_summaries(b_fc1)\n",
    "#convert pool layer into 1D\n",
    "    h_pool2_flat = tf.reshape(h_pool2,[-1, 3*3*32])\n",
    "\n",
    "# get first full-connect layer output\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "with tf.name_scope(\"dropout\"):\n",
    "# keep_prob and dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"softmax\"):\n",
    "# initial second full-connect layer weights and biases\n",
    "    with tf.name_scope(\"W_softmax\"):\n",
    "        W_fc2 = weight_variable([1000, 5])\n",
    "        variable_summaries(W_fc2)\n",
    "    with tf.name_scope(\"b_softmax\"):\n",
    "        b_fc2 = bias_variable([5])\n",
    "        variable_summaries(b_fc2)\n",
    "# finally output\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n",
    "# cross-entropy\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "with tf.name_scope(\"train\"):   \n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "with tf.name_scope(\"accuracy\"): \n",
    "# accuracy\n",
    "    with tf.name_scope(\"correct_prediction\"):  \n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "    with tf.name_scope(\"accuracy\"):  \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "# initial variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# make session execu\n",
    "merge = tf.summary.merge_all()\n",
    "train_acc_summary = tf.summary.scalar(\"train_accuracy\",accuracy)\n",
    "validation_acc_summary = tf.summary.scalar(\"validation\", accuracy)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(r\"G:\\Paper\\python\\TensorFlow\\test06\\logfile/cnn_11x11lr=1000=1e-4-2c_kp==0.5\", sess.graph)\n",
    "    best_validation_accuracy = 0.0\n",
    "    last_improvement = 0\n",
    "    require_improvement = 100\n",
    "    total_iterations = 0\n",
    "    for epoch in range(1000):\n",
    "        total_iterations += 1\n",
    "        for i in range(n_batch):\n",
    "            batch_x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            summary,_ = sess.run([merge,train_step], feed_dict={x:batch_x, y:batch_y, keep_prob:0.5})\n",
    "        train_acc_str = sess.run(train_acc_summary, feed_dict={x:x_train, y:y_train, keep_prob:1.0})\n",
    "        validation_acc_str = sess.run(validation_acc_summary,feed_dict={x:x_validation, y:y_validation, keep_prob:1.0})\n",
    "        writer.add_summary(train_acc_str,epoch)\n",
    "        writer.add_summary(validation_acc_str, epoch)\n",
    "        writer.add_summary(summary,epoch)\n",
    "        if (total_iterations % 10 == 0) or (epoch == 999):\n",
    "            train_acc = sess.run(accuracy, feed_dict={x:x_train, y:y_train, keep_prob:1.0})\n",
    "            validation_acc = sess.run(accuracy,feed_dict={x:x_validation, y:y_validation, keep_prob:1.0})\n",
    "            if validation_acc > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_acc\n",
    "                last_improvement = total_iterations\n",
    "                saver.save(sess,r\"G:\\Paper\\python\\TensorFlow\\test06\\cnn_11x11lr=1000=1e-4-2c_kp==0.5/my_net.ckpt\")\n",
    "                improved_str = \"*\"\n",
    "            else:\n",
    "                improved_str = \" \"\n",
    "            msg = \"Iter: {0:>6}, Train Accuracy: {1:>6.1%}, Validation Acc: {2:>6.1%} {3}\"\n",
    "            print(msg.format(epoch + 1, train_acc, validation_acc, improved_str))\n",
    "        if total_iterations - last_improvement > require_improvement:\n",
    "            print(\"No improvement found in a while, stopping optimization\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn-5 kp=0.5 390 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn-7 kp=0.5  380 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn-9 kp=0.5 290  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn-11 kp=0.5  260 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
