{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from osgeo import gdal, gdalconst\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = gdal.GetDriverByName(\"HFA\")\n",
    "driver.Register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 301 4\n",
      "(543616.671, 0.96, -0.0, 2952995.761, -0.0, -0.96)\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "needclass_file = r\"G:\\Paper\\JL\\GF-2_3000_GS_302.dat\"\n",
    "ds = gdal.Open(needclass_file)\n",
    "# getting dimensions\n",
    "cols = ds.RasterXSize\n",
    "rows = ds.RasterYSize\n",
    "bands = ds.RasterCount\n",
    "print(cols, rows, bands)\n",
    "geotransform = ds.GetGeoTransform()\n",
    "originX = geotransform[0]\n",
    "originY = geotransform[3]\n",
    "print(geotransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature files by block\n",
    "needclassdsbypixel = []\n",
    "m = 9\n",
    "for i in range(0, rows, 1):\n",
    "    if i + m <= rows:\n",
    "        for j in range(0, cols, 1):\n",
    "            if j + m <= cols:\n",
    "                data = ds.ReadAsArray(j, i, m, m).reshape(m*m*4,)/10000\n",
    "                needclassdsbypixel.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0465  0.047   0.0477  0.0488  0.0498  0.0498  0.0483  0.0473  0.0461\n",
      "  0.0471  0.0495  0.0509  0.051   0.0502  0.0486  0.0472  0.0459  0.0441\n",
      "  0.0458  0.0474  0.0489  0.0494  0.0485  0.0454  0.0435  0.0439  0.0455\n",
      "  0.047   0.0466  0.0472  0.0475  0.0459  0.042   0.0406  0.0424  0.0459\n",
      "  0.0494  0.0485  0.0493  0.049   0.0461  0.0429  0.0426  0.044   0.0451\n",
      "  0.0508  0.0499  0.0501  0.0491  0.0465  0.045   0.0459  0.0462  0.0444\n",
      "  0.0523  0.0505  0.0483  0.0462  0.0464  0.0481  0.0487  0.0466  0.0437\n",
      "  0.0528  0.0499  0.0463  0.0441  0.0465  0.0503  0.0498  0.0462  0.0436\n",
      "  0.0521  0.0501  0.0484  0.0464  0.0464  0.0476  0.047   0.0455  0.0447\n",
      "  0.0637  0.0646  0.0656  0.0674  0.0691  0.069   0.0667  0.0655  0.0642\n",
      "  0.0657  0.0701  0.0727  0.0726  0.0708  0.0678  0.0652  0.0633  0.0608\n",
      "  0.0643  0.0674  0.0703  0.0708  0.0686  0.0625  0.059   0.06    0.0632\n",
      "  0.0668  0.0663  0.0674  0.0676  0.064   0.0565  0.0536  0.057   0.0636\n",
      "  0.0715  0.0696  0.0706  0.0695  0.0637  0.0574  0.0566  0.0591  0.0612\n",
      "  0.0742  0.0716  0.0713  0.0688  0.0636  0.0602  0.0615  0.0621  0.0592\n",
      "  0.077   0.0722  0.0674  0.0629  0.0625  0.0651  0.0658  0.0621  0.0571\n",
      "  0.0778  0.071   0.0636  0.0588  0.0625  0.0685  0.0675  0.0613  0.0568\n",
      "  0.0764  0.0711  0.0667  0.0625  0.0618  0.0636  0.0624  0.06    0.0587\n",
      "  0.0484  0.0498  0.0513  0.0536  0.0558  0.0559  0.053   0.0511  0.0487\n",
      "  0.0494  0.0545  0.0578  0.0582  0.0569  0.0542  0.0515  0.0489  0.0451\n",
      "  0.0467  0.0502  0.0537  0.0551  0.0538  0.048   0.0445  0.0455  0.0485\n",
      "  0.0485  0.0482  0.05    0.0512  0.0485  0.0413  0.0386  0.0425  0.0494\n",
      "  0.053   0.0518  0.0539  0.0538  0.0486  0.0427  0.0425  0.0455  0.0475\n",
      "  0.0553  0.054   0.0551  0.0536  0.049   0.0465  0.0487  0.0495  0.0458\n",
      "  0.058   0.0547  0.051   0.0474  0.0483  0.0525  0.054   0.05    0.0442\n",
      "  0.0586  0.0531  0.0466  0.0427  0.0484  0.0566  0.0562  0.0493  0.044\n",
      "  0.057   0.0533  0.0501  0.0467  0.0474  0.0506  0.0502  0.0477  0.0463\n",
      "  0.3027  0.3057  0.3113  0.3229  0.3357  0.341   0.3357  0.3376  0.3415\n",
      "  0.3076  0.3264  0.3401  0.3442  0.3435  0.3382  0.3342  0.3339  0.3318\n",
      "  0.2967  0.3091  0.324   0.333   0.3329  0.3151  0.3085  0.3231  0.3495\n",
      "  0.3047  0.3013  0.3095  0.3182  0.3122  0.2885  0.2855  0.3119  0.3538\n",
      "  0.323   0.3168  0.3273  0.3307  0.3141  0.2957  0.3027  0.3241  0.3432\n",
      "  0.3325  0.3274  0.3339  0.3313  0.3173  0.3129  0.329   0.3396  0.3324\n",
      "  0.345   0.3321  0.3185  0.3065  0.3152  0.3383  0.3506  0.3393  0.3209\n",
      "  0.351   0.3289  0.3029  0.2891  0.3166  0.3555  0.3581  0.3334  0.316\n",
      "  0.349   0.3342  0.3223  0.309   0.3134  0.3293  0.3298  0.3225  0.3204] (324,)\n"
     ]
    }
   ],
   "source": [
    "print( needclassdsbypixel[0],  needclassdsbypixel[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from G:\\Paper\\python\\TensorFlow\\test06\\cnn_9x9lr=1000=1e-4-2c_kp==0.5/my_net.ckpt\n",
      "[[  8.39084672e-19   9.56588367e-04   5.11351449e-04   2.22618833e-11\n",
      "    9.98532057e-01]\n",
      " [  9.28398956e-20   4.17122897e-03   1.17073278e-03   9.34125867e-12\n",
      "    9.94658053e-01]\n",
      " [  3.11420511e-20   1.36385416e-03   9.54934570e-04   1.56043798e-12\n",
      "    9.97681141e-01]\n",
      " ..., \n",
      " [  2.10194356e-13   1.52006690e-13   9.99928117e-01   3.72752813e-18\n",
      "    7.18688971e-05]\n",
      " [  8.55841954e-14   6.04491717e-13   9.99545753e-01   1.46791769e-17\n",
      "    4.54222376e-04]\n",
      " [  1.30177729e-13   2.68159979e-13   9.99695301e-01   1.43803943e-17\n",
      "    3.04753601e-04]]\n"
     ]
    }
   ],
   "source": [
    "# define function\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(\"mean\", mean)\n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)\n",
    "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "        tf.summary.histogram(\"histogram\", var)\n",
    "\n",
    "# init weights function\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "#     initial = tf.contrib.layers.variance_scaling_initializer()\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# init biases function\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# define convo layer function\n",
    "def conv2d(x,W):\n",
    "    # x input tensor of shape [batch, in_height, in_width, in_channels]\n",
    "    # W filter/kernel tensor of shape [filter_height, filter_width, in_channels,out_channels]\n",
    "    #strides[0]=strides[3]=1,strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A \"string\" from:\"SAME\", \"VALID\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "def conv3d(x,W):\n",
    "    # x input tensor of shape [batch, in_depth,in_height, in_width, in_channels]\n",
    "    # W filter/kernel tensor of shape [filter_depth,filter_height, filter_width, in_channels,out_channels]\n",
    "    #strides[0]=strides[4]=1,strides[1]代表x方向的步长，strides[2]代表y方向的步长,strides[3]代表z方向的步长\n",
    "    #padding: A \"string\" from:\"SAME\", \"VALID\"\n",
    "    return tf.nn.conv3d(x, W, strides=[1, 1, 1, 1,1], padding=\"SAME\")\n",
    "# define pooling layer\n",
    "def max_pool_2x2(x):\n",
    "    #ksize(1, x, y, 1)\n",
    "    return tf.nn.max_pool(x,ksize=[1, 2, 2, 1], strides= [1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "# make placeholder\n",
    "    x = tf.placeholder(tf.float32, [None,m*m*4], name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None,5], name=\"y\")\n",
    "    x_image = tf.reshape(x, [-1, m, m, 4])\n",
    "\n",
    "with tf.name_scope(\"conv1\"):\n",
    "    # initial first conv layer weights and biases\n",
    "    with tf.name_scope(\"W_conv1\"):\n",
    "        W_conv1 = weight_variable([3, 3, 4, 16])\n",
    "        variable_summaries(W_conv1)\n",
    "    with tf.name_scope(\"b_conv1\"):\n",
    "        b_conv1 = bias_variable([16])\n",
    "        variable_summaries(b_conv1)\n",
    "    # x_image and weights biases  conv1, relu activation function\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    # initial second conv layer weights and biases\n",
    "    with tf.name_scope(\"W_conv2\"):\n",
    "        W_conv2 = weight_variable([3, 3, 16, 32])\n",
    "        variable_summaries(W_conv2)\n",
    "    with tf.name_scope(\"b_conv2\"):\n",
    "        b_conv2 = bias_variable([32])\n",
    "        variable_summaries(b_conv2)\n",
    "    # h_pool1 and weights biases  conv2, relu activation function\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2) \n",
    "with tf.name_scope(\"fc1\"):\n",
    "    # initial first full-connectelly layer weights and biases\n",
    "    with tf.name_scope(\"W_fc1\"):\n",
    "        W_fc1 = weight_variable([3*3*32, 1000])\n",
    "        variable_summaries(W_fc1)\n",
    "    with tf.name_scope(\"b_fc1\"):\n",
    "        b_fc1 = bias_variable([1000])\n",
    "        variable_summaries(b_fc1)\n",
    "    #convert pool layer into 1D\n",
    "    h_pool2_flat = tf.reshape(h_pool2,[-1, 3*3*32])\n",
    "\n",
    "    # get first full-connect layer output\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    # keep_prob and dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"softmax\"):\n",
    "    # initial second full-connect layer weights and biases\n",
    "    with tf.name_scope(\"W_softmax\"):\n",
    "        W_fc2 = weight_variable([1000, 5])\n",
    "        variable_summaries(W_fc2)\n",
    "    with tf.name_scope(\"b_softmax\"):\n",
    "        b_fc2 = bias_variable([5])\n",
    "        variable_summaries(b_fc2)\n",
    "    # finally output\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n",
    "# cross-entropy\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "with tf.name_scope(\"train\"):   \n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"): \n",
    "    # accuracy\n",
    "    with tf.name_scope(\"correct_prediction\"):  \n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "    with tf.name_scope(\"accuracy\"):  \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "# initial variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,r\"G:\\Paper\\python\\TensorFlow\\test06\\cnn_9x9lr=1000=1e-4-2c_kp==0.5/my_net.ckpt\")\n",
    "    da_pre = sess.run(prediction, feed_dict= {x:needclassdsbypixel,keep_prob:1.0})\n",
    "    print(da_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4, ..., 4, 4, 4],\n",
       "       [4, 4, 4, ..., 4, 4, 4],\n",
       "       [4, 4, 4, ..., 4, 4, 4],\n",
       "       ..., \n",
       "       [1, 1, 1, ..., 2, 2, 2],\n",
       "       [1, 1, 1, ..., 2, 2, 2],\n",
       "       [2, 1, 1, ..., 2, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_result1 = np.argmax(da_pre, axis=-1)\n",
    "class_result2 = class_result1.reshape(301-m+1, 301-m+1) \n",
    "class_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDataset = driver.Create(r\"G:\\Paper\\JL\\test_res\\302_cnn_9x9_1000_cl.img\",\n",
    "                          301-m+1, 301-m+1, 1, gdal.GDT_Int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 1, 1],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 1, 1],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 1, 1],\n",
       "       [4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 1],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 1, 1],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 2, 1, 1, 1],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1],\n",
       "       [4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outBand = outDataset.GetRasterBand(1)\n",
    "outBand.WriteArray(class_result2,0, 0)\n",
    "A = outBand.ReadAsArray(0,0, 20, 20)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 4.0, 2.4091369730573096, 1.3475545220881857]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outBand.FlushCache()\n",
    "outBand.GetStatistics(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoTransform = ds.GetGeoTransform()\n",
    "new_geoTransform=(geoTransform[0]+(0.96*(m-1)/2),geoTransform[1],geoTransform[2],geoTransform[3]-(0.96*(m-1)/2),geoTransform[4],geoTransform[5])\n",
    "outDataset.SetGeoTransform(new_geoTransform)\n",
    "proj = ds.GetProjection()\n",
    "outDataset.SetProjection(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building pyramids\n",
    "gdal.SetConfigOption(\"HFA_USE_RRD\", \"YES\")\n",
    "outDataset.BuildOverviews(overviewlist=[2, 4, 8, 16, 32, 64, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543620.5109999999\n",
      "PROJCS[\"WGS_1984_UTM_Zone_50N\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"WGS_1984\",SPHEROID[\"WGS_84\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",117.0],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1]]\n"
     ]
    }
   ],
   "source": [
    "geotransform = outDataset.GetGeoTransform()\n",
    "print(geotransform[0])\n",
    "proj = outDataset.GetProjection()\n",
    "print(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
